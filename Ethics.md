
# Ethics 

## Ethics in the workplace
### Google's AI Principles: We Won't Build Weapons 

- Google has announced a set of AI principles that will govern the company's use of artificial intelligence.
  These principles state that Google will not use AI to create or deploy weapons, and will only pursue AI projects 
  that are "socially beneficial" and "avoid causing overall harm."
- The principles were met with mixed reactions from Google employees. Some employees praised the company for taking a 
  stand against AI weapons, while others felt that the principles were too vague and did not go far enough.
- Google has said that it will continue to work with the US military on non-weapons projects, such as cybersecurity and 
  search and rescue. However, it is unclear whether Google will be able to work with the Pentagon on cloud computing 
  projects, such as the $10 billion JEDI contract, without violating its new AI principles.
- The company's decision to back away from Project Maven is a significant step, but it remains to be seen how Google 
  will implement its new AI principles in practice. The company will need to be more transparent about its AI projects 
  and work with independent experts to ensure that its AI is being used ethically.

## Ethics in Technology
### The Ethics of Self-Driving Cars

The paragraph discusses the ethical dilemma of self-driving cars and how they should be programmed to make decisions in life-or-death situations. There is no easy answer to this question, and any decision is likely to have unintended consequences. The author suggests that automakers need to be more transparent about their decision-making process and work with ethicists to develop guidelines for self-driving cars.

Specifically, the author raises the following questions:

 - Should self-driving cars be programmed to prioritize the lives of their passengers over anyone outside the car?
 - Should self-driving cars be designed to avoid accidents, but if an accident is inevitable, should they go for the 
   "smaller thing"?
 - What if the "smaller thing" is a child?

The author also cites a study by Azim Shariff, an assistant professor of psychology and social behavior at the 
University of California, Irvine, which found that people are generally in favor of self-driving cars that minimize the 
number of casualties in an accident, but they are less likely to buy a car that would sacrifice their own family 
members for the greater good.

The author concludes by saying that ethical problems surrounding self-driving cars are not just theoretical and that 
automakers need to take them seriously.
